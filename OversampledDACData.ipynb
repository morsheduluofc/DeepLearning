{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DACOverSampledF2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFbob27sRJ9-"
      },
      "source": [
        "# Deep Neural Network Classifer for DAC\n",
        "\n",
        "-- DAC is a challenge-response based behavioral authentication system. It constructs a profile for each user, based on their circle drawing activity, and stores it at the server. During an authentication session, the collected user data is compared against stored profile to make an authentication decision. DAC is implemented as an app for Android user. For DAC, we collected data from 199 Amazon Mechanical Turks (AMT) and dropped outlier data.\n",
        "\n",
        "-- We use DAC data to train a Deep Neural Network Classifer. The number of vectors in each DAC profile is around 140-180. The number of vector in each profile is not suffeceint to train a Deep Neural Network Classifer. We used oversampling technique to increase the profile size.\n",
        "\n",
        "-- We then estimat the accuracy of Deep Neural Network Classifer for DAC data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOvj_1ZoRNGw"
      },
      "source": [
        "#import all packages\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT-ZCJOASSa6",
        "outputId": "28e94651-f1fa-4b42-fd40-7bc8171da8af"
      },
      "source": [
        "#Read the normalized profile data\n",
        "with open('Data/AllOriginalNData.csv') as csvfile:\n",
        "    DataNSet = list(csv.reader(csvfile, delimiter=','))\n",
        "with open('Data/allUserIndx.csv') as csvfile:\n",
        "    dataIndex = list(csv.reader(csvfile, delimiter=','))\n",
        "print('Successfully read the data..')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully read data..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT-YpAh0-nby",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e6dc067b-aff6-41c8-bc5e-803caf97a6ea"
      },
      "source": [
        "#Use dataframe and set column name\n",
        "AllNDataSet = pd.DataFrame(DataNSet)\n",
        "AllNDataSet=AllNDataSet[1:]\n",
        "\n",
        "#print(dClass1.shape)\n",
        "#print(dClass2.shape)\n",
        "columns=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','F31','F32','F33','F34','F35','F36','F37','F38','F39',\n",
        "            'F40','F41','F42','F43','F44','F45','F46','F47','F48','F49','F50','F51','F52','F53','F54','F55','F56','F57','F58','F59','F60',\n",
        "            'F61','F62','F63','F64','F65','ID']\n",
        "AllNDataSet.columns= columns \n",
        "AllNDataSet.head()        "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>F9</th>\n",
              "      <th>F10</th>\n",
              "      <th>F11</th>\n",
              "      <th>F12</th>\n",
              "      <th>F13</th>\n",
              "      <th>F14</th>\n",
              "      <th>F15</th>\n",
              "      <th>F16</th>\n",
              "      <th>F17</th>\n",
              "      <th>F18</th>\n",
              "      <th>F19</th>\n",
              "      <th>F20</th>\n",
              "      <th>F21</th>\n",
              "      <th>F22</th>\n",
              "      <th>F23</th>\n",
              "      <th>F24</th>\n",
              "      <th>F25</th>\n",
              "      <th>F26</th>\n",
              "      <th>F27</th>\n",
              "      <th>F28</th>\n",
              "      <th>F29</th>\n",
              "      <th>F30</th>\n",
              "      <th>F31</th>\n",
              "      <th>F32</th>\n",
              "      <th>F33</th>\n",
              "      <th>F34</th>\n",
              "      <th>F35</th>\n",
              "      <th>F36</th>\n",
              "      <th>F37</th>\n",
              "      <th>F38</th>\n",
              "      <th>F39</th>\n",
              "      <th>F40</th>\n",
              "      <th>F41</th>\n",
              "      <th>F42</th>\n",
              "      <th>F43</th>\n",
              "      <th>F44</th>\n",
              "      <th>F45</th>\n",
              "      <th>F46</th>\n",
              "      <th>F47</th>\n",
              "      <th>F48</th>\n",
              "      <th>F49</th>\n",
              "      <th>F50</th>\n",
              "      <th>F51</th>\n",
              "      <th>F52</th>\n",
              "      <th>F53</th>\n",
              "      <th>F54</th>\n",
              "      <th>F55</th>\n",
              "      <th>F56</th>\n",
              "      <th>F57</th>\n",
              "      <th>F58</th>\n",
              "      <th>F59</th>\n",
              "      <th>F60</th>\n",
              "      <th>F61</th>\n",
              "      <th>F62</th>\n",
              "      <th>F63</th>\n",
              "      <th>F64</th>\n",
              "      <th>F65</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.16604197064368384</td>\n",
              "      <td>0.10739354674425546</td>\n",
              "      <td>0.07117497153391529</td>\n",
              "      <td>0.07505547065793473</td>\n",
              "      <td>0.24743462122664162</td>\n",
              "      <td>0.3260382179689564</td>\n",
              "      <td>0.21310360041812107</td>\n",
              "      <td>0.19763658854087585</td>\n",
              "      <td>0.1158279923000801</td>\n",
              "      <td>0.08569580311805454</td>\n",
              "      <td>0.1384705112143959</td>\n",
              "      <td>0.003269755254764108</td>\n",
              "      <td>0.05530476307645043</td>\n",
              "      <td>0.14940130463762327</td>\n",
              "      <td>0.06856542076580235</td>\n",
              "      <td>0.14518074268963163</td>\n",
              "      <td>0.02610263511317432</td>\n",
              "      <td>0.14653035157436328</td>\n",
              "      <td>0.0929591827294403</td>\n",
              "      <td>0.08182926910927268</td>\n",
              "      <td>0.10576925540749739</td>\n",
              "      <td>0.016564238514342147</td>\n",
              "      <td>0.12824121499699548</td>\n",
              "      <td>0.11969157311415193</td>\n",
              "      <td>0.0599925410906684</td>\n",
              "      <td>0.14362442609620243</td>\n",
              "      <td>0.08647461936569545</td>\n",
              "      <td>0.08886886976989032</td>\n",
              "      <td>0.110987034605165</td>\n",
              "      <td>0.08975844151593236</td>\n",
              "      <td>0.16428571428571426</td>\n",
              "      <td>0.1483870967741935</td>\n",
              "      <td>0.1405</td>\n",
              "      <td>0.1390515463917526</td>\n",
              "      <td>0.1422680412371134</td>\n",
              "      <td>0.13939393939393938</td>\n",
              "      <td>0.13142857142857142</td>\n",
              "      <td>0.1408163265306122</td>\n",
              "      <td>0.15862068965517243</td>\n",
              "      <td>0.16235294117647056</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.6468788332082889</td>\n",
              "      <td>0.6696648544564788</td>\n",
              "      <td>0.6697493120346343</td>\n",
              "      <td>0.6701413951105702</td>\n",
              "      <td>0.674386091215733</td>\n",
              "      <td>0.6713432729609438</td>\n",
              "      <td>0.6706849624068189</td>\n",
              "      <td>0.6712571719964209</td>\n",
              "      <td>0.6703389941338666</td>\n",
              "      <td>0.6517564276359328</td>\n",
              "      <td>0.1719431136871497</td>\n",
              "      <td>0.15809048311951834</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.195852534562212</td>\n",
              "      <td>0.2328767123287671</td>\n",
              "      <td>A3MC5OA9RXOOFH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.16604197064368384</td>\n",
              "      <td>0.10739354674425546</td>\n",
              "      <td>0.07117497153391529</td>\n",
              "      <td>0.07505547065793473</td>\n",
              "      <td>0.24743462122664162</td>\n",
              "      <td>0.3260382179689564</td>\n",
              "      <td>0.21310360041812107</td>\n",
              "      <td>0.19763658854087585</td>\n",
              "      <td>0.1158279923000801</td>\n",
              "      <td>0.08569580311805454</td>\n",
              "      <td>0.1384705112143959</td>\n",
              "      <td>0.003269755254764108</td>\n",
              "      <td>0.05530476307645043</td>\n",
              "      <td>0.14940130463762327</td>\n",
              "      <td>0.06856542076580235</td>\n",
              "      <td>0.14518074268963163</td>\n",
              "      <td>0.02610263511317432</td>\n",
              "      <td>0.14653035157436328</td>\n",
              "      <td>0.0929591827294403</td>\n",
              "      <td>0.08182926910927268</td>\n",
              "      <td>0.10576925540749739</td>\n",
              "      <td>0.016564238514342147</td>\n",
              "      <td>0.12824121499699548</td>\n",
              "      <td>0.11969157311415193</td>\n",
              "      <td>0.0599925410906684</td>\n",
              "      <td>0.14362442609620243</td>\n",
              "      <td>0.08647461936569545</td>\n",
              "      <td>0.08886886976989032</td>\n",
              "      <td>0.110987034605165</td>\n",
              "      <td>0.08975844151593236</td>\n",
              "      <td>0.16428571428571426</td>\n",
              "      <td>0.1483870967741935</td>\n",
              "      <td>0.1405</td>\n",
              "      <td>0.1390515463917526</td>\n",
              "      <td>0.1422680412371134</td>\n",
              "      <td>0.13939393939393938</td>\n",
              "      <td>0.13142857142857142</td>\n",
              "      <td>0.1408163265306122</td>\n",
              "      <td>0.15862068965517243</td>\n",
              "      <td>0.16235294117647056</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.6468788332082889</td>\n",
              "      <td>0.6696648544564788</td>\n",
              "      <td>0.6697493120346343</td>\n",
              "      <td>0.6701413951105702</td>\n",
              "      <td>0.674386091215733</td>\n",
              "      <td>0.6713432729609438</td>\n",
              "      <td>0.6706849624068189</td>\n",
              "      <td>0.6712571719964209</td>\n",
              "      <td>0.6703389941338666</td>\n",
              "      <td>0.6517564276359328</td>\n",
              "      <td>0.1719431136871497</td>\n",
              "      <td>0.15809048311951834</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.195852534562212</td>\n",
              "      <td>0.2328767123287671</td>\n",
              "      <td>A3MC5OA9RXOOFH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.1610295206920834</td>\n",
              "      <td>0.11353831190109935</td>\n",
              "      <td>0.5100645231253049</td>\n",
              "      <td>0.282824878150661</td>\n",
              "      <td>0.21343910175160907</td>\n",
              "      <td>0.041145842930822324</td>\n",
              "      <td>0.08664878450065668</td>\n",
              "      <td>0.052861946727078</td>\n",
              "      <td>0.18414169779620906</td>\n",
              "      <td>0.26048206550834674</td>\n",
              "      <td>0.1466122707530549</td>\n",
              "      <td>0.06422052953890241</td>\n",
              "      <td>0.0869815488443789</td>\n",
              "      <td>0.10349397339302614</td>\n",
              "      <td>0.12233872608366077</td>\n",
              "      <td>0.10723757039709736</td>\n",
              "      <td>0.12522340308637525</td>\n",
              "      <td>0.14380780614522695</td>\n",
              "      <td>0.15297768811546478</td>\n",
              "      <td>0.03711353158530151</td>\n",
              "      <td>0.10610351885044852</td>\n",
              "      <td>0.04143284335803052</td>\n",
              "      <td>0.049297444790368974</td>\n",
              "      <td>0.05887234697995661</td>\n",
              "      <td>0.08112580863970958</td>\n",
              "      <td>0.13645636142082415</td>\n",
              "      <td>0.038355652394103004</td>\n",
              "      <td>0.07623717272754561</td>\n",
              "      <td>0.15337579711698848</td>\n",
              "      <td>0.13596485833604502</td>\n",
              "      <td>0.16809523809523808</td>\n",
              "      <td>0.1483870967741935</td>\n",
              "      <td>0.14375</td>\n",
              "      <td>0.14556701030927835</td>\n",
              "      <td>0.20049484536082476</td>\n",
              "      <td>0.16476767676767676</td>\n",
              "      <td>0.18521904761904762</td>\n",
              "      <td>0.21126530612244893</td>\n",
              "      <td>0.26326436781609197</td>\n",
              "      <td>0.19934117647058822</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.20830357142857142</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.6371339083825356</td>\n",
              "      <td>0.6795576693545494</td>\n",
              "      <td>0.6796433746063462</td>\n",
              "      <td>0.6697389707044678</td>\n",
              "      <td>0.6695021932679076</td>\n",
              "      <td>0.6552628741773269</td>\n",
              "      <td>0.6510994121288122</td>\n",
              "      <td>0.6510994121288122</td>\n",
              "      <td>0.6568366650554972</td>\n",
              "      <td>0.6419380241483531</td>\n",
              "      <td>0.32150169120370553</td>\n",
              "      <td>0.1249702972940501</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1912442396313364</td>\n",
              "      <td>0.3287671232876712</td>\n",
              "      <td>A3MC5OA9RXOOFH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1610295206920834</td>\n",
              "      <td>0.11353831190109935</td>\n",
              "      <td>0.5100645231253049</td>\n",
              "      <td>0.282824878150661</td>\n",
              "      <td>0.21343910175160907</td>\n",
              "      <td>0.041145842930822324</td>\n",
              "      <td>0.08664878450065668</td>\n",
              "      <td>0.052861946727078</td>\n",
              "      <td>0.18414169779620906</td>\n",
              "      <td>0.26048206550834674</td>\n",
              "      <td>0.1466122707530549</td>\n",
              "      <td>0.06422052953890241</td>\n",
              "      <td>0.0869815488443789</td>\n",
              "      <td>0.10349397339302614</td>\n",
              "      <td>0.12233872608366077</td>\n",
              "      <td>0.10723757039709736</td>\n",
              "      <td>0.12522340308637525</td>\n",
              "      <td>0.14380780614522695</td>\n",
              "      <td>0.15297768811546478</td>\n",
              "      <td>0.03711353158530151</td>\n",
              "      <td>0.10610351885044852</td>\n",
              "      <td>0.04143284335803052</td>\n",
              "      <td>0.049297444790368974</td>\n",
              "      <td>0.05887234697995661</td>\n",
              "      <td>0.08112580863970958</td>\n",
              "      <td>0.13645636142082415</td>\n",
              "      <td>0.038355652394103004</td>\n",
              "      <td>0.07623717272754561</td>\n",
              "      <td>0.15337579711698848</td>\n",
              "      <td>0.13596485833604502</td>\n",
              "      <td>0.16809523809523808</td>\n",
              "      <td>0.1483870967741935</td>\n",
              "      <td>0.14375</td>\n",
              "      <td>0.14556701030927835</td>\n",
              "      <td>0.20049484536082476</td>\n",
              "      <td>0.16476767676767676</td>\n",
              "      <td>0.18521904761904762</td>\n",
              "      <td>0.21126530612244893</td>\n",
              "      <td>0.26326436781609197</td>\n",
              "      <td>0.19934117647058822</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.20830357142857142</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.6371339083825356</td>\n",
              "      <td>0.6795576693545494</td>\n",
              "      <td>0.6796433746063462</td>\n",
              "      <td>0.6697389707044678</td>\n",
              "      <td>0.6695021932679076</td>\n",
              "      <td>0.6552628741773269</td>\n",
              "      <td>0.6510994121288122</td>\n",
              "      <td>0.6510994121288122</td>\n",
              "      <td>0.6568366650554972</td>\n",
              "      <td>0.6419380241483531</td>\n",
              "      <td>0.32150169120370553</td>\n",
              "      <td>0.1249702972940501</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1912442396313364</td>\n",
              "      <td>0.3287671232876712</td>\n",
              "      <td>A3MC5OA9RXOOFH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.08692643468696541</td>\n",
              "      <td>0.04457041955137371</td>\n",
              "      <td>0.026819931681396732</td>\n",
              "      <td>0.045133367384287075</td>\n",
              "      <td>0.1819558913544829</td>\n",
              "      <td>0.09097907696213625</td>\n",
              "      <td>0.04671286340760411</td>\n",
              "      <td>0.19331223795265484</td>\n",
              "      <td>0.18772870428172553</td>\n",
              "      <td>0.3240984085267922</td>\n",
              "      <td>0.09898722741391291</td>\n",
              "      <td>0.026658393657795747</td>\n",
              "      <td>0.06222183184196131</td>\n",
              "      <td>0.16586201310150467</td>\n",
              "      <td>0.005638298387282079</td>\n",
              "      <td>0.01850722093955098</td>\n",
              "      <td>0.021098253431435163</td>\n",
              "      <td>0.03902740324466424</td>\n",
              "      <td>0.08835629301627261</td>\n",
              "      <td>0.03623960394918992</td>\n",
              "      <td>0.07149147017314106</td>\n",
              "      <td>0.0003987319383644454</td>\n",
              "      <td>0.10123591909223893</td>\n",
              "      <td>0.06864638027970606</td>\n",
              "      <td>0.032764359504133686</td>\n",
              "      <td>0.0585599378932636</td>\n",
              "      <td>0.025947103911612902</td>\n",
              "      <td>0.07492326239800991</td>\n",
              "      <td>0.07865306577211888</td>\n",
              "      <td>0.06108712199646097</td>\n",
              "      <td>0.19047619047619047</td>\n",
              "      <td>0.15182795698924728</td>\n",
              "      <td>0.14708333333333334</td>\n",
              "      <td>0.1487835051546392</td>\n",
              "      <td>0.16494845360824745</td>\n",
              "      <td>0.16476767676767676</td>\n",
              "      <td>0.22110476190476192</td>\n",
              "      <td>0.23371428571428568</td>\n",
              "      <td>0.21636781609195405</td>\n",
              "      <td>0.21778823529411764</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.11901785714285713</td>\n",
              "      <td>0.1488392857142857</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.20830357142857142</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.17857142857142858</td>\n",
              "      <td>0.6341022920676245</td>\n",
              "      <td>0.6557637761798153</td>\n",
              "      <td>0.6537428877220708</td>\n",
              "      <td>0.6557717358745551</td>\n",
              "      <td>0.6555398963708366</td>\n",
              "      <td>0.6435541030726061</td>\n",
              "      <td>0.6413167929750516</td>\n",
              "      <td>0.6413167929750516</td>\n",
              "      <td>0.6454775204503302</td>\n",
              "      <td>0.6359308024401372</td>\n",
              "      <td>0.20511594208797512</td>\n",
              "      <td>0.08014524289471224</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3202764976958525</td>\n",
              "      <td>0.0547945205479452</td>\n",
              "      <td>A3MC5OA9RXOOFH</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    F1                   F2  ...                 F65              ID\n",
              "1  0.16604197064368384  0.10739354674425546  ...  0.2328767123287671  A3MC5OA9RXOOFH\n",
              "2  0.16604197064368384  0.10739354674425546  ...  0.2328767123287671  A3MC5OA9RXOOFH\n",
              "3   0.1610295206920834  0.11353831190109935  ...  0.3287671232876712  A3MC5OA9RXOOFH\n",
              "4   0.1610295206920834  0.11353831190109935  ...  0.3287671232876712  A3MC5OA9RXOOFH\n",
              "5  0.08692643468696541  0.04457041955137371  ...  0.0547945205479452  A3MC5OA9RXOOFH\n",
              "\n",
              "[5 rows x 66 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYpVcujYToBN",
        "outputId": "741c3c72-8a7e-4ee6-cffb-769586215520"
      },
      "source": [
        "#Estimate the size of each profile in DAC\n",
        "AllNDataSet.groupby('ID').size()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "A3MC5OA9RXOOFH         180\n",
              "Aron                   140\n",
              "Athena                 140\n",
              "Avrahman               100\n",
              "COPIEDA5H767UHFV6JC    140\n",
              "                      ... \n",
              "vasmturk               140\n",
              "venkat                 140\n",
              "walmnh                 140\n",
              "wws1985                140\n",
              "yash                   140\n",
              "Length: 193, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VAixsXL_vSH"
      },
      "source": [
        "# Replace the user name by class label and seperate the data in majority and minority class \n",
        "Identity=AllNDataSet['ID']\n",
        "for i in range (0,195):\n",
        "  indx1=int(float(dataIndex[0][i]))\n",
        "  indx2=int(float(dataIndex[0][i+1]))\n",
        "  #print(indx1)\n",
        "  Identity[indx1:indx2]=i+1\n",
        "\n",
        "AllNDataSet=AllNDataSet.drop(['ID'], axis=1)\n",
        "\n",
        "AllNT1DataSet, AllNT2DataSet,IDT1,IDT2 = train_test_split(AllNDataSet, Identity, test_size=0.1, random_state=22)\n",
        "\n",
        "IDT1=IDT1.astype(int)\n",
        "IDT2=IDT2.astype(int)\n",
        "\n",
        "AllNT1DataSet=pd.concat([IDT1,AllNT1DataSet], axis=1, join='inner')\n",
        "AllNT2DataSet=pd.concat([IDT2,AllNT2DataSet], axis=1, join='inner')\n",
        "\n",
        "AllNT1DataSet = AllNT1DataSet.sort_values('ID')\n",
        "AllNT2DataSet = AllNT2DataSet.sort_values('ID')\n",
        "\n",
        "AllNT1DataSet.reset_index(drop=True, inplace=True)\n",
        "AllNT2DataSet.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac31bQsPDvCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a111b1-41b3-4ac0-a0ca-ee4567881fac"
      },
      "source": [
        "#print the size of majority and minority class\n",
        "#print(AllNDataSet.shape)\n",
        "#print(dfdataSet)\n",
        "AllNT2DataSet.groupby('ID').size()\n",
        "#print(AllNT1DataSet)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "1      23\n",
              "2      13\n",
              "3      10\n",
              "4      21\n",
              "5      11\n",
              "       ..\n",
              "191    21\n",
              "192     6\n",
              "193    12\n",
              "194    11\n",
              "195    13\n",
              "Length: 195, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJL3h-_hD4wG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37bda52-f49d-489e-b8fb-58cf7c469962"
      },
      "source": [
        "#print the indices of all training proilfes \n",
        "DataIndex=AllNT1DataSet.groupby('ID').size().values\n",
        "#print(DataIndex)\n",
        "i=range(len(DataIndex)+1)\n",
        "FDataIndex = array(i)\n",
        "\n",
        "for i in range(len(DataIndex)):\n",
        "  FDataIndex[i+1]=FDataIndex[i]+DataIndex[i]\n",
        " \n",
        "#print(DataIndex)\n",
        "print(FDataIndex)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0   157   284   414   513   642   765   857   913  1032  1157  1304\n",
            "  1392  1516  1640  1769  1890  2020  2148  2279  2410  2533  2661  2781\n",
            "  2907  3037  3162  3290  3416  3543  3674  3804  3912  4040  4165  4307\n",
            "  4432  4553  4679  4792  4954  5075  5198  5318  5408  5531  5599  5723\n",
            "  5847  5975  6028  6157  6280  6408  6538  6667  6793  6923  7089  7221\n",
            "  7345  7468  7597  7740  7798  7851  7945  8072  8200  8325  8434  8562\n",
            "  8687  8810  8954  9071  9196  9325  9415  9470  9526  9599  9765  9891\n",
            "  9961 10106 10226 10347 10468 10521 10647 10714 10806 10880 11004 11135\n",
            " 11278 11463 11592 11718 11845 11970 12074 12202 12325 12399 12523 12650\n",
            " 12700 12757 12884 12936 12990 13122 13252 13375 13506 13626 13748 13866\n",
            " 13992 14066 14193 14371 14514 14822 14964 15053 15181 15303 15447 15576\n",
            " 15704 15832 15919 16011 16138 16214 16287 16415 16596 16758 16884 16971\n",
            " 17096 17224 17348 17472 17616 17704 17809 17934 18062 18185 18277 18399\n",
            " 18529 18660 18782 18915 19043 19117 19192 19314 19439 19563 19690 19814\n",
            " 19945 20035 20161 20236 20310 20380 20470 20598 20718 20838 20969 21100\n",
            " 21229 21356 21480 21584 21638 21764 21854 21981 22140 22297 22421 22540\n",
            " 22674 22802 22931 23058]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-BLShiZERij"
      },
      "source": [
        "# Oversampled the data of each training profile. We limit 1000 samples for minority (training profile) class\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "AllDataSet=AllNT1DataSet\n",
        "length2 = len(AllDataSet)\n",
        "#print(length2)\n",
        "#minClass=AllDataSet[0:1]\n",
        "#majClass=AllDataSet[0:1]\n",
        "\n",
        "for indx in range(0,195):\n",
        "  \n",
        "  index1=int(float(FDataIndex[indx]))\n",
        "  name=AllDataSet.at[index1, 'ID']\n",
        "  #print(name)\n",
        "  \n",
        "  index2=int(float(FDataIndex[indx+1]))\n",
        "  minClass=AllDataSet[index1:index2]\n",
        "  majClass=AllDataSet[0:index1].append(AllDataSet[index2:length2])\n",
        "  #print(majClass)\n",
        "  index1=index2\n",
        "  #print(len(minClass))\n",
        "  #print(len(majClass))\n",
        "  #print(len(minClass[0]))\n",
        "\n",
        "  dfminClass = pd.DataFrame(minClass)\n",
        "\n",
        "  dfmajClass = pd.DataFrame(majClass)\n",
        "  dfminClass[\"ID\"]=0\n",
        "  dfmajClass[\"ID\"]=1\n",
        "  #print(\"User %s has %s samples\" %(name,len(dfminClass)))\n",
        "  #Class=dfminClass.append(dfmajClass.sample(n=1000)) \n",
        "  Class=dfminClass.append(dfmajClass.sample(n=1000))\n",
        "  #print(len(Class))\n",
        "  columns=Class.columns.tolist()\n",
        "  columns=[c for c in columns if c not in [\"ID\"]]\n",
        "  target=\"ID\"\n",
        "  state=np.random.RandomState(42)\n",
        "  X=Class[columns]\n",
        "  Y=Class[target]\n",
        "  #sns.countplot('ID', data = Class)\n",
        "  smk=SMOTETomek(random_state=42)\n",
        "  X_res,y_res=smk.fit_sample(X,Y)\n",
        "  #ada=ADASYN(random_state=42)\n",
        "  #X_res,y_res = ada.fit_resample(X,Y)\n",
        "  #print(X_res.shape,' ',y_res.shape)\n",
        "  #FClass=np.concatenate((y_res,X_res),axis=1)\n",
        "  #print(FClass.shape)\n",
        "  #print(X_res.shape,y_res.shape)\n",
        "  FClass=np.column_stack((X_res,y_res))\n",
        "  dFClass = pd.DataFrame(FClass)\n",
        "  dFClass.columns=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','F31','F32','F33','F34','F35','F36','F37','F38','F39',\n",
        "            'F40','F41','F42','F43','F44','F45','F46','F47','F48','F49','F50','F51','F52','F53','F54','F55','F56','F57','F58','F59','F60',\n",
        "            'F61','F62','F63','F64','F65','ID']\n",
        "  dFClass=dFClass[dFClass['ID'] != 1]\n",
        "  dFClass=dFClass.replace({'ID':{0:name}})\n",
        "  dFClass.reindex(np.random.permutation(dFClass.index))\n",
        "  if indx==0:\n",
        "   AllClass=dFClass\n",
        "  else:\n",
        "    AllClass=pd.concat([AllClass,dFClass])\n",
        "AllClass.index = range(AllClass.shape[0])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8FeWYpBE0Im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d0d2ee-a7b9-4064-9630-af7990d545f7"
      },
      "source": [
        "#print the size of the oversampled data\n",
        "#print(AllClass.shape)\n",
        "#print(dfdataSet)\n",
        "AllClass.groupby('ID').size()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "1.0      1000\n",
              "2.0      1000\n",
              "3.0      1000\n",
              "4.0      1000\n",
              "5.0      1000\n",
              "         ... \n",
              "191.0    1000\n",
              "192.0    1000\n",
              "193.0    1000\n",
              "194.0    1000\n",
              "195.0    1000\n",
              "Length: 195, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xndyTKlJHJXo"
      },
      "source": [
        "#Seperate the class label and data of each profile\n",
        "dfdataSet=AllClass\n",
        "\n",
        "columnsN=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','F31','F32','F33','F34','F35','F36','F37','F38','F39',\n",
        "            'F40','F41','F42','F43','F44','F45','F46','F47','F48','F49','F50','F51','F52','F53','F54','F55','F56','F57','F58','F59','F60',\n",
        "            'F61','F62','F63','F64','F65','ID']\n",
        "\n",
        "fdataSet = pd.DataFrame(columns = columnsN)\n",
        "\n",
        "#print(fdataSet)\n",
        "for i in range (0,195):\n",
        "  #fdataSet=fdataSet.append(shuffle(dfdataSet[1000*(i-1):i*1000]),ignore_index = True)\n",
        "  fdataSet=fdataSet.append(dfdataSet[1000*i:(i+1)*1000])\n",
        "\n",
        "fDataSet=fdataSet.drop(columns=['ID'])\n",
        "#fDataSet=standardize(fDataSet,columns=columnsF)\n",
        "\n",
        "fIDSet = pd.DataFrame(columns = ['ID'])\n",
        "fIDSet=fdataSet['ID']\n",
        "for i in range (0,195):\n",
        "  fIDSet[1000*i:(i+1)*1000]=i\n",
        "\n",
        "#fDataSet['ID'] = fIDSet"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY8MjhtbrWRv",
        "outputId": "fe0aca64-b89b-46f8-c7b2-d804a02fe634"
      },
      "source": [
        "#install tensorflow\n",
        "!pip install tensorflow"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_cwjfUAHdB4"
      },
      "source": [
        "#Seperate the oversampled data in [tranning set, validation set and test set] of all users\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "X=fDataSet\n",
        "y=fIDSet\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=22)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGKepUN_HiKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad6d7e9-cf83-47fb-c6d6-d2272963f660"
      },
      "source": [
        "#Print the sahpe of tranning data and id\n",
        "print(X_train.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175500, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBt82nOaHlMb"
      },
      "source": [
        "#import necessary packages for deep neural network\n",
        "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
        "from keras.models import Model,Sequential\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY7CCEjmHoKm"
      },
      "source": [
        "#define the optimizers\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "def adam_optimizer():\n",
        "    return Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "def RMSprop_optimizer():\n",
        "    return RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIbeEphOHuak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7382cf83-5e36-4d95-8afd-75b4c2b79bdb"
      },
      "source": [
        "#Construct a classifier for the initial experiments\n",
        "\n",
        "def create_classifier(release=False,Tuser=195):\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Dense(256, input_dim=65))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #classifier.add(Dense(256))\n",
        "  #classifier.add(BatchNormalization())\n",
        "  #classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(512))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(512))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #classifier.add(Dense(256))\n",
        "  #classifier.add(BatchNormalization())\n",
        "  #classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(256))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #if release:\n",
        "  classifier.add(Dense(Tuser, activation='softmax'))\n",
        "  #else:\n",
        "  #   classifier.add(Dense(Tuser))\n",
        "  #np.log_softmax_v2(a, axis=axis)\n",
        "  #classifier.add(F.softmax(a, dim=1))\n",
        "\n",
        "  #classifier.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer(),metrics=['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "Clasf=create_classifier()\n",
        "Clasf.summary()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               16896     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 195)               50115     \n",
            "=================================================================\n",
            "Total params: 598,723\n",
            "Trainable params: 595,651\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxd_Y1gdH4-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e3803a-5c79-4546-fa91-3bcb4390f7c4"
      },
      "source": [
        "# Train the Neural Network Classifer\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
        "callbacks_list = [learning_rate_reduction]\n",
        "\n",
        "Classfier= create_classifier(True,195)\n",
        "\n",
        "lossc='categorical_crossentropy'\n",
        "optimizerc=RMSprop(lr=0.001, rho=0.9)\n",
        "Classfier.compile(loss=lossc, optimizer='Adam',metrics=['accuracy'])\n",
        "\n",
        "history1 =  Classfier.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_val, y_val),verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2743/2743 [==============================] - 39s 14ms/step - loss: 1.9611 - accuracy: 0.3960 - val_loss: 0.9079 - val_accuracy: 0.6606\n",
            "Epoch 2/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.7256 - accuracy: 0.7333 - val_loss: 0.5336 - val_accuracy: 0.8070\n",
            "Epoch 3/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.3566 - accuracy: 0.8752 - val_loss: 0.3429 - val_accuracy: 0.8818\n",
            "Epoch 4/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.2386 - accuracy: 0.9174 - val_loss: 0.2344 - val_accuracy: 0.9200\n",
            "Epoch 5/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.1741 - accuracy: 0.9396 - val_loss: 0.1975 - val_accuracy: 0.9317\n",
            "Epoch 6/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.1518 - accuracy: 0.9465 - val_loss: 0.1521 - val_accuracy: 0.9483\n",
            "Epoch 7/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.1411 - accuracy: 0.9504 - val_loss: 0.1112 - val_accuracy: 0.9598\n",
            "Epoch 8/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.1126 - accuracy: 0.9603 - val_loss: 0.1222 - val_accuracy: 0.9586\n",
            "Epoch 9/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.1081 - accuracy: 0.9622 - val_loss: 0.0860 - val_accuracy: 0.9697\n",
            "Epoch 10/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.1049 - accuracy: 0.9626 - val_loss: 0.1303 - val_accuracy: 0.9553\n",
            "Epoch 11/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0982 - accuracy: 0.9643 - val_loss: 0.0847 - val_accuracy: 0.9692\n",
            "Epoch 12/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0889 - accuracy: 0.9671 - val_loss: 0.1241 - val_accuracy: 0.9555\n",
            "Epoch 13/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0828 - accuracy: 0.9698 - val_loss: 0.0779 - val_accuracy: 0.9712\n",
            "Epoch 14/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0825 - accuracy: 0.9696 - val_loss: 0.0762 - val_accuracy: 0.9730\n",
            "Epoch 15/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0797 - accuracy: 0.9706 - val_loss: 0.0719 - val_accuracy: 0.9736\n",
            "Epoch 16/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0719 - accuracy: 0.9726 - val_loss: 0.0630 - val_accuracy: 0.9770\n",
            "Epoch 17/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0753 - accuracy: 0.9721 - val_loss: 0.0651 - val_accuracy: 0.9745\n",
            "Epoch 18/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0691 - accuracy: 0.9744 - val_loss: 0.0677 - val_accuracy: 0.9750\n",
            "Epoch 19/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0715 - accuracy: 0.9729 - val_loss: 0.0608 - val_accuracy: 0.9768\n",
            "Epoch 20/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0636 - accuracy: 0.9753 - val_loss: 0.0590 - val_accuracy: 0.9772\n",
            "Epoch 21/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0650 - accuracy: 0.9740 - val_loss: 0.0581 - val_accuracy: 0.9775\n",
            "Epoch 22/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0647 - accuracy: 0.9756 - val_loss: 0.0424 - val_accuracy: 0.9822\n",
            "Epoch 23/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0583 - accuracy: 0.9774 - val_loss: 0.0398 - val_accuracy: 0.9836\n",
            "Epoch 24/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0672 - accuracy: 0.9751 - val_loss: 0.0507 - val_accuracy: 0.9807\n",
            "Epoch 25/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0590 - accuracy: 0.9772 - val_loss: 0.0439 - val_accuracy: 0.9823\n",
            "Epoch 26/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0571 - accuracy: 0.9775 - val_loss: 0.0447 - val_accuracy: 0.9824\n",
            "Epoch 27/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0500 - accuracy: 0.9806 - val_loss: 0.0440 - val_accuracy: 0.9820\n",
            "Epoch 28/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0542 - accuracy: 0.9794 - val_loss: 0.0430 - val_accuracy: 0.9821\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 29/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0344 - accuracy: 0.9846 - val_loss: 0.0269 - val_accuracy: 0.9872\n",
            "Epoch 30/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0305 - accuracy: 0.9865 - val_loss: 0.0280 - val_accuracy: 0.9862\n",
            "Epoch 31/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0301 - accuracy: 0.9864 - val_loss: 0.0279 - val_accuracy: 0.9870\n",
            "Epoch 32/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0308 - accuracy: 0.9865 - val_loss: 0.0261 - val_accuracy: 0.9875\n",
            "Epoch 33/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0297 - accuracy: 0.9870 - val_loss: 0.0284 - val_accuracy: 0.9874\n",
            "Epoch 34/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0295 - accuracy: 0.9868 - val_loss: 0.0273 - val_accuracy: 0.9872\n",
            "Epoch 35/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0309 - accuracy: 0.9867 - val_loss: 0.0270 - val_accuracy: 0.9881\n",
            "Epoch 36/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0286 - accuracy: 0.9870 - val_loss: 0.0302 - val_accuracy: 0.9859\n",
            "Epoch 37/50\n",
            "2743/2743 [==============================] - 37s 13ms/step - loss: 0.0320 - accuracy: 0.9863 - val_loss: 0.0254 - val_accuracy: 0.9880\n",
            "Epoch 38/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0304 - accuracy: 0.9863 - val_loss: 0.0298 - val_accuracy: 0.9862\n",
            "Epoch 39/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0324 - accuracy: 0.9861 - val_loss: 0.0274 - val_accuracy: 0.9873\n",
            "Epoch 40/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0287 - accuracy: 0.9870 - val_loss: 0.0263 - val_accuracy: 0.9882\n",
            "Epoch 41/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0287 - accuracy: 0.9864 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
            "Epoch 42/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0271 - accuracy: 0.9875 - val_loss: 0.0259 - val_accuracy: 0.9880\n",
            "Epoch 43/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0277 - accuracy: 0.9873 - val_loss: 0.0244 - val_accuracy: 0.9878\n",
            "Epoch 44/50\n",
            "2743/2743 [==============================] - 39s 14ms/step - loss: 0.0288 - accuracy: 0.9871 - val_loss: 0.0264 - val_accuracy: 0.9873\n",
            "Epoch 45/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0285 - accuracy: 0.9876 - val_loss: 0.0251 - val_accuracy: 0.9888\n",
            "Epoch 46/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0274 - accuracy: 0.9875 - val_loss: 0.0255 - val_accuracy: 0.9886\n",
            "Epoch 47/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0310 - accuracy: 0.9863 - val_loss: 0.0260 - val_accuracy: 0.9880\n",
            "Epoch 48/50\n",
            "2743/2743 [==============================] - 37s 14ms/step - loss: 0.0279 - accuracy: 0.9874 - val_loss: 0.0257 - val_accuracy: 0.9876\n",
            "Epoch 49/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0286 - accuracy: 0.9874 - val_loss: 0.0268 - val_accuracy: 0.9872\n",
            "Epoch 50/50\n",
            "2743/2743 [==============================] - 38s 14ms/step - loss: 0.0253 - accuracy: 0.9883 - val_loss: 0.0271 - val_accuracy: 0.9866\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNw4Cy5_ICra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "d8c9b87c-bf9b-4043-9cc2-2e47df32d401"
      },
      "source": [
        "# Plot the classifier loss and accuracy curves for the training and validation data\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(history1.history['loss'], color='b', label=\"Training loss\")\n",
        "ax[0].plot(history1.history['val_loss'], color='r', label=\"Validation loss\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "ax[0].set(xlabel='epochs', ylabel='loss')\n",
        "\n",
        "\n",
        "ax[1].plot(history1.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax[1].plot(history1.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)\n",
        "ax[1].set(xlabel='epochs', ylabel='accuracy')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'accuracy'), Text(0.5, 0, 'epochs')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHoy6t4TIGx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b755ab93-fbb6-4e14-872a-504f1cf51a11"
      },
      "source": [
        "#Calculate the accuracy for test data\n",
        "y_test=AllNT2DataSet['ID']\n",
        "X_test=AllNT2DataSet.drop(['ID'],axis=1).astype(float)\n",
        "y_test=y_test.astype(int)-1\n",
        "y_test = to_categorical(y_test)\n",
        "print(y_test)\n",
        "Classfier.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
        "loss, accuracy = Classfier.evaluate(X_test, y_test)\n",
        "#print('Test score:', score)\n",
        "print('Accuracy:', accuracy)\n",
        "print('Loss:', loss)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "81/81 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.9324\n",
            "Accuracy: 0.9153005480766296\n",
            "Loss: 0.6649223566055298\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}