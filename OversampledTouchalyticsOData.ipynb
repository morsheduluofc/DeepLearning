{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TouchalyticsOverSampled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybuO-t3Hv47S"
      },
      "source": [
        "# Deep Neural Network Classifer for Touchalytics\n",
        "\n",
        "-- Touchalytics uses users' touch data (up-down and left-right scrolling) when interacting with an app. It uses the collected touch data for user authentication. The system uses 30 behavioral features and data from 41 users. We downloaded touchalytics data from the link http://www.mariofrank.net/touchalytics/. We then cleaned the data by replacing 'NaN' and 'Infinity' by zero and dropped the 'doc id', 'phone id', and 'change of finger orientation' columns.\n",
        "\n",
        "-- We use Touchalytics data to train a Deep Neural Network Classifer. The number of vectors in each Touchalytics profile varies around 300-1230. The number of vector in some profile is not suffeceint to train a Deep Neural Network Classifer. We used oversampling technique to increase their profile size.\n",
        "\n",
        "-- We then estimat the accuracy of Deep Neural Network Classifer for Touchalytics data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbwY-lIbv2fR"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "import warnings"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m-nlqSswq20",
        "outputId": "d9a7146c-cfe5-4ddb-edd9-a23788734069"
      },
      "source": [
        "#Read the normalized profile data\n",
        "with open('Data/AllMargeTNorData.csv') as csvfile:\n",
        "    DataNSet = list(csv.reader(csvfile, delimiter=','))\n",
        "with open('Data/allUserIndxRP.csv') as csvfile:\n",
        "    dataIndex = list(csv.reader(csvfile, delimiter=','))\n",
        "print('Successfully read all data..')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully read all data..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Aec4yvvbq3t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "56308caf-6fef-49b5-c758-7f4046d0bcb6"
      },
      "source": [
        "#Use dataframe and set column name\n",
        "AllNDataSet = pd.DataFrame(DataNSet)\n",
        "AllNDataSet=AllNDataSet[1:]\n",
        "AllNDataSet=pd.concat([AllNDataSet.iloc[:,1:31], AllNDataSet.iloc[:,0:1]], axis=1, join='inner')\n",
        "#print(AllNDataSet)\n",
        "#print(dClass1.shape)\n",
        "#print(dClass2.shape)\n",
        "columns=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','ID']\n",
        "AllNDataSet.columns= columns \n",
        "AllNDataSet.head() "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>F9</th>\n",
              "      <th>F10</th>\n",
              "      <th>F11</th>\n",
              "      <th>F12</th>\n",
              "      <th>F13</th>\n",
              "      <th>F14</th>\n",
              "      <th>F15</th>\n",
              "      <th>F16</th>\n",
              "      <th>F17</th>\n",
              "      <th>F18</th>\n",
              "      <th>F19</th>\n",
              "      <th>F20</th>\n",
              "      <th>F21</th>\n",
              "      <th>F22</th>\n",
              "      <th>F23</th>\n",
              "      <th>F24</th>\n",
              "      <th>F25</th>\n",
              "      <th>F26</th>\n",
              "      <th>F27</th>\n",
              "      <th>F28</th>\n",
              "      <th>F29</th>\n",
              "      <th>F30</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.5000011873786407</td>\n",
              "      <td>0.006498745069917532</td>\n",
              "      <td>0.12030002586429865</td>\n",
              "      <td>0.08184426811816052</td>\n",
              "      <td>0.4790425730021504</td>\n",
              "      <td>0.20684197211795513</td>\n",
              "      <td>0.40342734074747966</td>\n",
              "      <td>0.98692</td>\n",
              "      <td>0.3333333333333333</td>\n",
              "      <td>0.5559768760351638</td>\n",
              "      <td>0.0961835467860379</td>\n",
              "      <td>0.03586765397855321</td>\n",
              "      <td>0.02713461726463975</td>\n",
              "      <td>0.846777208976157</td>\n",
              "      <td>0.27360910244786946</td>\n",
              "      <td>0.10612872390705001</td>\n",
              "      <td>0.018935337675524407</td>\n",
              "      <td>0.4638459649697168</td>\n",
              "      <td>0.7050138805348634</td>\n",
              "      <td>0.5775460518810888</td>\n",
              "      <td>0.1865308576738762</td>\n",
              "      <td>0.5458435480276354</td>\n",
              "      <td>0.055127493057308755</td>\n",
              "      <td>0.99509</td>\n",
              "      <td>0.037796848981683104</td>\n",
              "      <td>0.27314811423390756</td>\n",
              "      <td>0.5500945604825557</td>\n",
              "      <td>0.465107225531523</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5000005097087379</td>\n",
              "      <td>0.006185012549300824</td>\n",
              "      <td>0.12781963962410553</td>\n",
              "      <td>0.3732611297218833</td>\n",
              "      <td>0.3956261208288585</td>\n",
              "      <td>0.3391415229148292</td>\n",
              "      <td>0.2795714895750172</td>\n",
              "      <td>0.98265</td>\n",
              "      <td>0.3333333333333333</td>\n",
              "      <td>0.4927616575359919</td>\n",
              "      <td>0.13617245716294984</td>\n",
              "      <td>0.033839371950766486</td>\n",
              "      <td>0.007142438165998632</td>\n",
              "      <td>0.8483821528751753</td>\n",
              "      <td>0.27376733998186764</td>\n",
              "      <td>0.10598683733753446</td>\n",
              "      <td>0.032601104534535276</td>\n",
              "      <td>0.4812127306316208</td>\n",
              "      <td>0.7119094989126914</td>\n",
              "      <td>0.5935518240514704</td>\n",
              "      <td>0.20399427852991017</td>\n",
              "      <td>0.4960560667324652</td>\n",
              "      <td>0.03871749558192376</td>\n",
              "      <td>0.98188</td>\n",
              "      <td>0.02786473677468938</td>\n",
              "      <td>0.2737682447869447</td>\n",
              "      <td>0.42155014919185735</td>\n",
              "      <td>0.27907940507934653</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.5000009334951456</td>\n",
              "      <td>0.039485478666188595</td>\n",
              "      <td>0.5112854556427279</td>\n",
              "      <td>0.5329398461108044</td>\n",
              "      <td>0.48460366981370323</td>\n",
              "      <td>0.5981512248818877</td>\n",
              "      <td>0.11328790776257404</td>\n",
              "      <td>0.73295</td>\n",
              "      <td>0.6666666666666666</td>\n",
              "      <td>0.8019811440947893</td>\n",
              "      <td>0.005422690450752606</td>\n",
              "      <td>0.0017998464548403873</td>\n",
              "      <td>6.660475119757552E-4</td>\n",
              "      <td>0.8484983436185134</td>\n",
              "      <td>0.27379864817951044</td>\n",
              "      <td>0.1059631752658527</td>\n",
              "      <td>9.217910299908369E-4</td>\n",
              "      <td>0.46314325468278655</td>\n",
              "      <td>0.704087817517235</td>\n",
              "      <td>0.5719899158130091</td>\n",
              "      <td>0.18524620542628595</td>\n",
              "      <td>0.7661975866789774</td>\n",
              "      <td>0.016818985104771523</td>\n",
              "      <td>0.91592</td>\n",
              "      <td>0.0019306007429230178</td>\n",
              "      <td>0.2738045679057117</td>\n",
              "      <td>0.37334599495784543</td>\n",
              "      <td>0.23255151974347923</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.500000454854369</td>\n",
              "      <td>0.009053424166367873</td>\n",
              "      <td>0.5545133201138029</td>\n",
              "      <td>0.2934126166875886</td>\n",
              "      <td>0.28438717818553483</td>\n",
              "      <td>0.24783090271367908</td>\n",
              "      <td>0.30309500380609344</td>\n",
              "      <td>0.75255</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.015336348579436845</td>\n",
              "      <td>0.08282055096493246</td>\n",
              "      <td>0.019677802818296636</td>\n",
              "      <td>0.005714634861667807</td>\n",
              "      <td>0.8483843267882188</td>\n",
              "      <td>0.27371514324569357</td>\n",
              "      <td>0.1059599184718393</td>\n",
              "      <td>0.01659897471457936</td>\n",
              "      <td>0.4840036947828731</td>\n",
              "      <td>0.7119094989126914</td>\n",
              "      <td>0.5944244940795327</td>\n",
              "      <td>0.2090195149505293</td>\n",
              "      <td>0.035674488204017946</td>\n",
              "      <td>0.04222166119666751</td>\n",
              "      <td>0.97614</td>\n",
              "      <td>0.020899449212245423</td>\n",
              "      <td>0.27373175249320036</td>\n",
              "      <td>0.5340265090712183</td>\n",
              "      <td>0.3720933153054348</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.49921</td>\n",
              "      <td>0.003944065973467192</td>\n",
              "      <td>0.40225881541512204</td>\n",
              "      <td>0.43313378223785304</td>\n",
              "      <td>0.1712962372462537</td>\n",
              "      <td>0.3503203221682084</td>\n",
              "      <td>0.26806037763873675</td>\n",
              "      <td>0.57212</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0357529621607848</td>\n",
              "      <td>0.08840458190768566</td>\n",
              "      <td>0.0342207583149657</td>\n",
              "      <td>0.037554501906344705</td>\n",
              "      <td>0.8452655680224404</td>\n",
              "      <td>0.27360768812330005</td>\n",
              "      <td>0.11315478534856242</td>\n",
              "      <td>0.03329948735728968</td>\n",
              "      <td>0.4865713350326217</td>\n",
              "      <td>0.7119094989126914</td>\n",
              "      <td>0.5892975291460382</td>\n",
              "      <td>0.2142235017583499</td>\n",
              "      <td>0.06429685758858927</td>\n",
              "      <td>0.037997980308003027</td>\n",
              "      <td>0.95929</td>\n",
              "      <td>0.042369668246445495</td>\n",
              "      <td>0.27360768812330005</td>\n",
              "      <td>0.4938563805428751</td>\n",
              "      <td>0.4186002704184789</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   F1                    F2                   F3  ...  F29  F30 ID\n",
              "1  0.5000011873786407  0.006498745069917532  0.12030002586429865  ...  0.5  0.0  1\n",
              "2  0.5000005097087379  0.006185012549300824  0.12781963962410553  ...  0.5  0.0  1\n",
              "3  0.5000009334951456  0.039485478666188595   0.5112854556427279  ...  0.5  0.0  1\n",
              "4   0.500000454854369  0.009053424166367873   0.5545133201138029  ...  0.5  0.0  1\n",
              "5             0.49921  0.003944065973467192  0.40225881541512204  ...  0.5  0.0  1\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRKyMYQNbvDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91402bb9-99e7-494f-d268-10b16f8c584d"
      },
      "source": [
        "#Estimate the size of each profile in Touchalytics\n",
        "AllNDataSet.groupby('ID').size()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "1      400\n",
              "10     386\n",
              "11     444\n",
              "12     342\n",
              "13     304\n",
              "14     650\n",
              "15     718\n",
              "16     382\n",
              "17     706\n",
              "18     460\n",
              "19     292\n",
              "2     1230\n",
              "20     356\n",
              "21     702\n",
              "22     374\n",
              "23     968\n",
              "24     434\n",
              "25     432\n",
              "26     242\n",
              "27     608\n",
              "28     710\n",
              "29     438\n",
              "3      758\n",
              "30     224\n",
              "31     370\n",
              "32     304\n",
              "33     712\n",
              "34     612\n",
              "35    1062\n",
              "36     500\n",
              "37     424\n",
              "38     786\n",
              "39     436\n",
              "4      240\n",
              "40     312\n",
              "41     186\n",
              "5      412\n",
              "6      562\n",
              "7      590\n",
              "8      618\n",
              "9      452\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1T94TyNbyAv"
      },
      "source": [
        "# Replace the user name by class label and seperate the data in majority and minority class \n",
        "Identity=AllNDataSet['ID']\n",
        "AllNDataSet=AllNDataSet.drop(['ID'], axis=1)\n",
        "\n",
        "AllNT1DataSet, AllNT2DataSet,IDT1,IDT2 = train_test_split(AllNDataSet, Identity, test_size=0.1, random_state=22)\n",
        "IDT1=IDT1.astype(int)\n",
        "IDT2=IDT2.astype(int)\n",
        "\n",
        "AllNT1DataSet=pd.concat([IDT1,AllNT1DataSet], axis=1, join='inner')\n",
        "AllNT2DataSet=pd.concat([IDT2,AllNT2DataSet], axis=1, join='inner')\n",
        "\n",
        "AllNT1DataSet = AllNT1DataSet.sort_values('ID')\n",
        "AllNT2DataSet = AllNT2DataSet.sort_values('ID')\n",
        "\n",
        "AllNT1DataSet.reset_index(drop=True, inplace=True)\n",
        "AllNT2DataSet.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcHBcxCQm9Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6248a6-8064-47a2-eadf-0ce1086cf0cf"
      },
      "source": [
        "#print the size of majority and minority class\n",
        "#print(AllNDataSet.shape)\n",
        "#print(dfdataSet)\n",
        "AllNT1DataSet.groupby('ID').size()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "1      359\n",
              "2     1108\n",
              "3      686\n",
              "4      218\n",
              "5      375\n",
              "6      502\n",
              "7      531\n",
              "8      550\n",
              "9      403\n",
              "10     346\n",
              "11     381\n",
              "12     307\n",
              "13     272\n",
              "14     588\n",
              "15     663\n",
              "16     343\n",
              "17     626\n",
              "18     416\n",
              "19     260\n",
              "20     312\n",
              "21     629\n",
              "22     342\n",
              "23     865\n",
              "24     386\n",
              "25     385\n",
              "26     225\n",
              "27     541\n",
              "28     646\n",
              "29     398\n",
              "30     205\n",
              "31     337\n",
              "32     276\n",
              "33     643\n",
              "34     558\n",
              "35     960\n",
              "36     450\n",
              "37     389\n",
              "38     698\n",
              "39     393\n",
              "40     286\n",
              "41     166\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyYhkE4NdkLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0826971d-1f9b-481f-d4d7-534be1a152cf"
      },
      "source": [
        "#print the indices of all training proilfes \n",
        "DataIndex=AllNT1DataSet.groupby('ID').size().values\n",
        "#print(DataIndex)\n",
        "i=range(len(DataIndex)+1)\n",
        "FDataIndex = array(i)\n",
        "\n",
        "for i in range(len(DataIndex)):\n",
        "  FDataIndex[i+1]=FDataIndex[i]+DataIndex[i]\n",
        "\n",
        "print(FDataIndex)\n",
        "#print(AllNT2DataSet[0:43])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0   359  1467  2153  2371  2746  3248  3779  4329  4732  5078  5459\n",
            "  5766  6038  6626  7289  7632  8258  8674  8934  9246  9875 10217 11082\n",
            " 11468 11853 12078 12619 13265 13663 13868 14205 14481 15124 15682 16642\n",
            " 17092 17481 18179 18572 18858 19024]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeunEsdLeh27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104b6d8b-08b3-419d-958b-c1ccfb7ca839"
      },
      "source": [
        "# Oversampled the data of each training profile. We limit 2000 samples for minority (training profile) class\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Special line for normalized and non-normalized data\n",
        "AllDataSet=AllNT1DataSet\n",
        "length2 = len(AllDataSet)\n",
        "print(length2)\n",
        "#minClass=AllDataSet[0:1]\n",
        "#majClass=AllDataSet[0:1]\n",
        "\n",
        "for indx in range(0,41):\n",
        "  index1=int(float(FDataIndex[indx]))\n",
        "  name=AllDataSet.at[index1,'ID']\n",
        "  #print(name)\n",
        "  \n",
        "  index2=int(float(FDataIndex[indx+1]))\n",
        "  minClass=AllDataSet[index1:index2]\n",
        "  majClass=AllDataSet[0:index1].append(AllDataSet[index2:length2])\n",
        "  #print(majClass)\n",
        "  index1=index2\n",
        "  #print(len(minClass))\n",
        "  #print(len(majClass))\n",
        "  #print(len(minClass[0]))\n",
        "\n",
        "  #join majority and minority class in single class\n",
        "  dfminClass = pd.DataFrame(minClass)\n",
        "  dfmajClass = pd.DataFrame(majClass)\n",
        "\n",
        "  #df.rename(columns={'0':'ID'})\n",
        "  dfminClass[\"ID\"]=0\n",
        "  dfmajClass[\"ID\"]=1\n",
        "  \n",
        "  #print(dfminClass)\n",
        "  #print(\"User %s has %s samples\" %(name,len(dfminClass)))\n",
        "  #Class=dfminClass.append(dfmajClass.sample(n=1000))\n",
        "  if(len(dfminClass)>=2000):\n",
        "      Class=dfminClass\n",
        "      columns=Class.columns.tolist()\n",
        "      columns=[c for c in columns if c not in [\"ID\"]]\n",
        "      target=\"ID\"\n",
        "      state=np.random.RandomState(42)\n",
        "      X=Class[columns]\n",
        "      Y=Class[target]\n",
        "      FClass=np.column_stack((X,Y))\n",
        "    \n",
        "      dFClass = pd.DataFrame(FClass)\n",
        "      dFClass.columns=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','ID']\n",
        "      dFClass=dFClass.replace({'ID':{0:name}})\n",
        "      dFClass.reindex(np.random.permutation(dFClass.index))\n",
        "      AllClass=pd.concat([AllClass,dFClass.sample(n=2000)]) \n",
        "      \n",
        "  if(len(dfminClass)<2000): \n",
        "    #print(indx+1)\n",
        "    Class=dfminClass.append(dfmajClass.sample(n=2000))\n",
        "    #print(len(dfminClass))\n",
        "    #print(len(Class))\n",
        "    #print(len(Class))\n",
        "    columns=Class.columns.tolist()\n",
        "    columns=[c for c in columns if c not in [\"ID\"]]\n",
        "    target=\"ID\"\n",
        "    state=np.random.RandomState(42)\n",
        "    X=Class[columns]\n",
        "    Y=Class[target]\n",
        "    \n",
        "    #print(FClass)\n",
        "    #sns.countplot('ID', data = Class)\n",
        "    smk=SMOTETomek(random_state=42)\n",
        "    X_res,y_res=smk.fit_sample(X,Y)\n",
        "    \n",
        "    #ada=ADASYN(random_state=42)\n",
        "    #X_res,y_res = ada.fit_resample(X,Y)\n",
        "    #print(X_res.shape,' ',y_res.shape)\n",
        "    #FClass=np.concatenate((y_res,X_res),axis=1)\n",
        "    #print(FClass.shape)\n",
        "    #print(X_res.shape,y_res.shape)\n",
        "    FClass=np.column_stack((X_res,y_res))\n",
        "    \n",
        "    dFClass = pd.DataFrame(FClass)\n",
        "    dFClass.columns=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','ID']\n",
        "    dFClass=dFClass[dFClass['ID'] != 1]\n",
        "    dFClass=dFClass.replace({'ID':{0:name}})\n",
        "    dFClass.reindex(np.random.permutation(dFClass.index))\n",
        "    if(len(dFClass)<2000):\n",
        "      diff=2000-len(dFClass)\n",
        "      dFClass=pd.concat([dFClass,dFClass.sample(n=diff)])\n",
        "\n",
        "    if indx==0:\n",
        "      AllClass=dFClass\n",
        "    else:\n",
        "      AllClass=pd.concat([AllClass,dFClass])\n",
        "\n",
        "AllClass.index = range(AllClass.shape[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjtSBR_OeuSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef42045-0469-4be7-8333-3e39156b4d21"
      },
      "source": [
        "#print the size of the oversampled data\n",
        "#print(AllClass)\n",
        "#print(dfdataSet)\n",
        "AllClass.groupby('ID').size()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "1.0     2000\n",
              "2.0     2000\n",
              "3.0     2000\n",
              "4.0     2000\n",
              "5.0     2000\n",
              "6.0     2000\n",
              "7.0     2000\n",
              "8.0     2000\n",
              "9.0     2000\n",
              "10.0    2000\n",
              "11.0    2000\n",
              "12.0    2000\n",
              "13.0    2000\n",
              "14.0    2000\n",
              "15.0    2000\n",
              "16.0    2000\n",
              "17.0    2000\n",
              "18.0    2000\n",
              "19.0    2000\n",
              "20.0    2000\n",
              "21.0    2000\n",
              "22.0    2000\n",
              "23.0    2000\n",
              "24.0    2000\n",
              "25.0    2000\n",
              "26.0    2000\n",
              "27.0    2000\n",
              "28.0    2000\n",
              "29.0    2000\n",
              "30.0    2000\n",
              "31.0    2000\n",
              "32.0    2000\n",
              "33.0    2000\n",
              "34.0    2000\n",
              "35.0    2000\n",
              "36.0    2000\n",
              "37.0    2000\n",
              "38.0    2000\n",
              "39.0    2000\n",
              "40.0    2000\n",
              "41.0    2000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2BqRnhHpzp3"
      },
      "source": [
        "#Seperate the class label and data of each profile\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "dfdataSet=AllClass\n",
        "\n",
        "columnsN=['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
        "            'F20','F21','F22','F23','F24','F25','F26','F27','F28','F29','F30','ID']\n",
        "\n",
        "fdataSet = pd.DataFrame(columns = columnsN)\n",
        "\n",
        "#print(fdataSet)\n",
        "for i in range (0,41):\n",
        "  #fdataSet=fdataSet.append(shuffle(dfdataSet[1000*(i-1):i*1000]),ignore_index = True)\n",
        "  fdataSet=fdataSet.append(dfdataSet[2000*i+1:(i+1)*2000+1])\n",
        "\n",
        "fDataSet=fdataSet.drop(columns=['ID'])\n",
        "#fDataSet=standardize(fDataSet,columns=columnsF)\n",
        "\n",
        "fIDSet = pd.DataFrame(columns = ['ID'])\n",
        "fIDSet=fdataSet['ID']\n",
        "for i in range (0,41):\n",
        "  fIDSet[2000*i:(i+1)*2000]=i\n",
        "\n",
        "#fDataSet['ID'] = fIDSet"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQsGhQCyUFs",
        "outputId": "81ee6a54-7746-4199-c635-344d5c91c6ff"
      },
      "source": [
        "#install tensorflow\n",
        "!pip install tensorflow"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (57.0.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLFei1b8q0DD"
      },
      "source": [
        "#Seperate the oversampled data in [tranning set, validation set and test set] of all users\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "X=fDataSet\n",
        "y=fIDSet\n",
        "\n",
        "#print(y1.shape)\n",
        "#print(X2)\n",
        "#print(y2)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=22)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocBQD6HbxZd6"
      },
      "source": [
        "#import necessary packages for deep neural network\n",
        "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
        "from keras.models import Model,Sequential\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5lahRCnxcIl"
      },
      "source": [
        "#define the optimizers\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "def adam_optimizer():\n",
        "    return Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "def RMSprop_optimizer():\n",
        "    return RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMvNOOJdz8EI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6dcdec-34fd-4f62-c6e2-1905d2d73ce9"
      },
      "source": [
        "#Construct a classifier for the initial experiments\n",
        "\n",
        "def create_classifier(release=False,Tuser=41):\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Dense(64, input_dim=30))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(128))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(256))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  #classifier.add(Dense(512))\n",
        "  #classifier.add(BatchNormalization())\n",
        "  #classifier.add(Activation('tanh'))\n",
        "\n",
        "  #classifier.add(Dense(512))\n",
        "  #classifier.add(BatchNormalization())\n",
        "  #classifier.add(Activation('tanh'))\n",
        "\n",
        "  classifier.add(Dense(256))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(128))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(64))\n",
        "  classifier.add(BatchNormalization())\n",
        "  classifier.add(Activation('relu'))\n",
        "\n",
        "  classifier.add(Dense(Tuser, activation='softmax'))\n",
        "  \n",
        "  #np.log_softmax_v2(a, axis=axis)\n",
        "  #classifier.add(F.softmax(a, dim=1))\n",
        "\n",
        "  #classifier.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer(),metrics=['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "Clasf=create_classifier()\n",
        "Clasf.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1984      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 41)                2665      \n",
            "=================================================================\n",
            "Total params: 156,521\n",
            "Trainable params: 154,729\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WShx0J5v0BCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5564c6-fc92-472a-b531-4d06c45d0bbd"
      },
      "source": [
        "# Train the Neural Network Classifer\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
        "callbacks_list = [learning_rate_reduction]\n",
        "\n",
        "Classfier= create_classifier(True,41)\n",
        "\n",
        "#------Comment will start from here\n",
        "lossc='categorical_crossentropy'\n",
        "optimizerc=RMSprop(lr=0.001, rho=0.9)\n",
        "Classfier.compile(loss=lossc, optimizer='Adam',metrics=['accuracy'])\n",
        "#------Comments will end from here\n",
        "history1 =  Classfier.fit(X_train, y_train, batch_size=64, epochs=100, validation_data=(X_val, y_val),verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1154/1154 [==============================] - 23s 6ms/step - loss: 1.8506 - accuracy: 0.5096 - val_loss: 1.0741 - val_accuracy: 0.6650\n",
            "Epoch 2/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.8210 - accuracy: 0.7411 - val_loss: 0.8045 - val_accuracy: 0.7398\n",
            "Epoch 3/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.6751 - accuracy: 0.7833 - val_loss: 0.6624 - val_accuracy: 0.7907\n",
            "Epoch 4/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.5883 - accuracy: 0.8101 - val_loss: 0.6402 - val_accuracy: 0.7932\n",
            "Epoch 5/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.5300 - accuracy: 0.8270 - val_loss: 0.5409 - val_accuracy: 0.8228\n",
            "Epoch 6/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.4984 - accuracy: 0.8373 - val_loss: 0.8298 - val_accuracy: 0.7434\n",
            "Epoch 7/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.4622 - accuracy: 0.8481 - val_loss: 0.5480 - val_accuracy: 0.8251\n",
            "Epoch 8/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.4226 - accuracy: 0.8615 - val_loss: 0.3872 - val_accuracy: 0.8745\n",
            "Epoch 9/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.4125 - accuracy: 0.8642 - val_loss: 0.5506 - val_accuracy: 0.8300\n",
            "Epoch 10/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.3889 - accuracy: 0.8715 - val_loss: 0.5224 - val_accuracy: 0.8378\n",
            "Epoch 11/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.3639 - accuracy: 0.8792 - val_loss: 0.4885 - val_accuracy: 0.8479\n",
            "Epoch 12/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.3535 - accuracy: 0.8846 - val_loss: 0.5657 - val_accuracy: 0.8255\n",
            "Epoch 13/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.3285 - accuracy: 0.8925 - val_loss: 0.5208 - val_accuracy: 0.8367\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 14/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2735 - accuracy: 0.9098 - val_loss: 0.2405 - val_accuracy: 0.9260\n",
            "Epoch 15/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2433 - accuracy: 0.9188 - val_loss: 0.2399 - val_accuracy: 0.9255\n",
            "Epoch 16/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2360 - accuracy: 0.9202 - val_loss: 0.2458 - val_accuracy: 0.9226\n",
            "Epoch 17/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2334 - accuracy: 0.9220 - val_loss: 0.2173 - val_accuracy: 0.9356\n",
            "Epoch 18/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2091 - accuracy: 0.9301 - val_loss: 0.2210 - val_accuracy: 0.9339\n",
            "Epoch 19/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2171 - accuracy: 0.9273 - val_loss: 0.2030 - val_accuracy: 0.9388\n",
            "Epoch 20/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2103 - accuracy: 0.9298 - val_loss: 0.2056 - val_accuracy: 0.9401\n",
            "Epoch 21/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.2013 - accuracy: 0.9320 - val_loss: 0.2018 - val_accuracy: 0.9424\n",
            "Epoch 22/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1986 - accuracy: 0.9333 - val_loss: 0.2502 - val_accuracy: 0.9234\n",
            "Epoch 23/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1949 - accuracy: 0.9338 - val_loss: 0.1892 - val_accuracy: 0.9451\n",
            "Epoch 24/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1890 - accuracy: 0.9359 - val_loss: 0.1872 - val_accuracy: 0.9463\n",
            "Epoch 25/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1818 - accuracy: 0.9396 - val_loss: 0.1943 - val_accuracy: 0.9429\n",
            "Epoch 26/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1940 - accuracy: 0.9351 - val_loss: 0.1938 - val_accuracy: 0.9433\n",
            "Epoch 27/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1787 - accuracy: 0.9398 - val_loss: 0.2317 - val_accuracy: 0.9322\n",
            "Epoch 28/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1841 - accuracy: 0.9372 - val_loss: 0.2531 - val_accuracy: 0.9211\n",
            "Epoch 29/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1808 - accuracy: 0.9388 - val_loss: 0.1945 - val_accuracy: 0.9474\n",
            "Epoch 30/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1744 - accuracy: 0.9422 - val_loss: 0.2252 - val_accuracy: 0.9337\n",
            "Epoch 31/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1670 - accuracy: 0.9437 - val_loss: 0.1846 - val_accuracy: 0.9477\n",
            "Epoch 32/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1606 - accuracy: 0.9468 - val_loss: 0.1950 - val_accuracy: 0.9434\n",
            "Epoch 33/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1740 - accuracy: 0.9410 - val_loss: 0.1866 - val_accuracy: 0.9490\n",
            "Epoch 34/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1780 - accuracy: 0.9400 - val_loss: 0.2127 - val_accuracy: 0.9382\n",
            "Epoch 35/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1692 - accuracy: 0.9431 - val_loss: 0.1934 - val_accuracy: 0.9435\n",
            "Epoch 36/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1526 - accuracy: 0.9503 - val_loss: 0.1747 - val_accuracy: 0.9530\n",
            "Epoch 37/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1661 - accuracy: 0.9422 - val_loss: 0.1932 - val_accuracy: 0.9480\n",
            "Epoch 38/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1683 - accuracy: 0.9427 - val_loss: 0.1784 - val_accuracy: 0.9526\n",
            "Epoch 39/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1589 - accuracy: 0.9459 - val_loss: 0.1949 - val_accuracy: 0.9445\n",
            "Epoch 40/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1584 - accuracy: 0.9461 - val_loss: 0.1783 - val_accuracy: 0.9490\n",
            "Epoch 41/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1518 - accuracy: 0.9490 - val_loss: 0.1705 - val_accuracy: 0.9555\n",
            "Epoch 42/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1597 - accuracy: 0.9450 - val_loss: 0.1796 - val_accuracy: 0.9520\n",
            "Epoch 43/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1471 - accuracy: 0.9493 - val_loss: 0.1881 - val_accuracy: 0.9470\n",
            "Epoch 44/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1508 - accuracy: 0.9494 - val_loss: 0.1855 - val_accuracy: 0.9487\n",
            "Epoch 45/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1530 - accuracy: 0.9482 - val_loss: 0.1873 - val_accuracy: 0.9485\n",
            "Epoch 46/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1522 - accuracy: 0.9482 - val_loss: 0.1716 - val_accuracy: 0.9552\n",
            "\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 47/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1209 - accuracy: 0.9586 - val_loss: 0.1263 - val_accuracy: 0.9729\n",
            "Epoch 48/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1114 - accuracy: 0.9620 - val_loss: 0.1287 - val_accuracy: 0.9682\n",
            "Epoch 49/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1108 - accuracy: 0.9616 - val_loss: 0.1292 - val_accuracy: 0.9710\n",
            "Epoch 50/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1066 - accuracy: 0.9642 - val_loss: 0.1323 - val_accuracy: 0.9706\n",
            "Epoch 51/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1029 - accuracy: 0.9658 - val_loss: 0.1237 - val_accuracy: 0.9718\n",
            "Epoch 52/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.1011 - accuracy: 0.9657 - val_loss: 0.1291 - val_accuracy: 0.9702\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 53/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0931 - accuracy: 0.9683 - val_loss: 0.1177 - val_accuracy: 0.9755\n",
            "Epoch 54/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0845 - accuracy: 0.9715 - val_loss: 0.1149 - val_accuracy: 0.9760\n",
            "Epoch 55/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0885 - accuracy: 0.9701 - val_loss: 0.1180 - val_accuracy: 0.9744\n",
            "Epoch 56/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0890 - accuracy: 0.9701 - val_loss: 0.1127 - val_accuracy: 0.9755\n",
            "Epoch 57/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0787 - accuracy: 0.9728 - val_loss: 0.1142 - val_accuracy: 0.9749\n",
            "Epoch 58/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0788 - accuracy: 0.9734 - val_loss: 0.1156 - val_accuracy: 0.9754\n",
            "Epoch 59/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0839 - accuracy: 0.9720 - val_loss: 0.1166 - val_accuracy: 0.9765\n",
            "Epoch 60/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0825 - accuracy: 0.9722 - val_loss: 0.1145 - val_accuracy: 0.9755\n",
            "Epoch 61/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0794 - accuracy: 0.9739 - val_loss: 0.1156 - val_accuracy: 0.9761\n",
            "Epoch 62/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0804 - accuracy: 0.9729 - val_loss: 0.1134 - val_accuracy: 0.9759\n",
            "Epoch 63/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0754 - accuracy: 0.9743 - val_loss: 0.1145 - val_accuracy: 0.9750\n",
            "Epoch 64/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0806 - accuracy: 0.9728 - val_loss: 0.1160 - val_accuracy: 0.9759\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 65/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0734 - accuracy: 0.9752 - val_loss: 0.1137 - val_accuracy: 0.9774\n",
            "Epoch 66/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0736 - accuracy: 0.9749 - val_loss: 0.1153 - val_accuracy: 0.9760\n",
            "Epoch 67/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0774 - accuracy: 0.9741 - val_loss: 0.1179 - val_accuracy: 0.9754\n",
            "Epoch 68/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0766 - accuracy: 0.9741 - val_loss: 0.1169 - val_accuracy: 0.9759\n",
            "Epoch 69/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0758 - accuracy: 0.9740 - val_loss: 0.1174 - val_accuracy: 0.9754\n",
            "Epoch 70/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0717 - accuracy: 0.9754 - val_loss: 0.1165 - val_accuracy: 0.9761\n",
            "Epoch 71/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0724 - accuracy: 0.9762 - val_loss: 0.1171 - val_accuracy: 0.9761\n",
            "Epoch 72/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0698 - accuracy: 0.9771 - val_loss: 0.1130 - val_accuracy: 0.9763\n",
            "Epoch 73/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0755 - accuracy: 0.9746 - val_loss: 0.1127 - val_accuracy: 0.9773\n",
            "Epoch 74/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0681 - accuracy: 0.9768 - val_loss: 0.1131 - val_accuracy: 0.9762\n",
            "Epoch 75/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0720 - accuracy: 0.9754 - val_loss: 0.1123 - val_accuracy: 0.9763\n",
            "Epoch 76/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0692 - accuracy: 0.9765 - val_loss: 0.1109 - val_accuracy: 0.9760\n",
            "Epoch 77/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0747 - accuracy: 0.9737 - val_loss: 0.1104 - val_accuracy: 0.9766\n",
            "Epoch 78/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0699 - accuracy: 0.9765 - val_loss: 0.1085 - val_accuracy: 0.9772\n",
            "Epoch 79/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0696 - accuracy: 0.9757 - val_loss: 0.1107 - val_accuracy: 0.9767\n",
            "Epoch 80/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0727 - accuracy: 0.9743 - val_loss: 0.1142 - val_accuracy: 0.9763\n",
            "Epoch 81/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0719 - accuracy: 0.9758 - val_loss: 0.1091 - val_accuracy: 0.9773\n",
            "Epoch 82/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0731 - accuracy: 0.9745 - val_loss: 0.1100 - val_accuracy: 0.9763\n",
            "Epoch 83/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0722 - accuracy: 0.9756 - val_loss: 0.1122 - val_accuracy: 0.9762\n",
            "Epoch 84/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0679 - accuracy: 0.9774 - val_loss: 0.1117 - val_accuracy: 0.9765\n",
            "Epoch 85/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.1157 - val_accuracy: 0.9760\n",
            "Epoch 86/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0668 - accuracy: 0.9768 - val_loss: 0.1124 - val_accuracy: 0.9771\n",
            "Epoch 87/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0724 - accuracy: 0.9746 - val_loss: 0.1145 - val_accuracy: 0.9746\n",
            "Epoch 88/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0673 - accuracy: 0.9760 - val_loss: 0.1097 - val_accuracy: 0.9771\n",
            "Epoch 89/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0706 - accuracy: 0.9751 - val_loss: 0.1131 - val_accuracy: 0.9773\n",
            "Epoch 90/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0700 - accuracy: 0.9755 - val_loss: 0.1116 - val_accuracy: 0.9776\n",
            "Epoch 91/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0678 - accuracy: 0.9768 - val_loss: 0.1134 - val_accuracy: 0.9767\n",
            "Epoch 92/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0713 - accuracy: 0.9761 - val_loss: 0.1125 - val_accuracy: 0.9773\n",
            "Epoch 93/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0654 - accuracy: 0.9782 - val_loss: 0.1110 - val_accuracy: 0.9774\n",
            "Epoch 94/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0704 - accuracy: 0.9769 - val_loss: 0.1147 - val_accuracy: 0.9774\n",
            "Epoch 95/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0670 - accuracy: 0.9771 - val_loss: 0.1129 - val_accuracy: 0.9771\n",
            "Epoch 96/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0663 - accuracy: 0.9772 - val_loss: 0.1114 - val_accuracy: 0.9771\n",
            "Epoch 97/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0671 - accuracy: 0.9771 - val_loss: 0.1134 - val_accuracy: 0.9767\n",
            "Epoch 98/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0667 - accuracy: 0.9766 - val_loss: 0.1136 - val_accuracy: 0.9763\n",
            "Epoch 99/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.1117 - val_accuracy: 0.9774\n",
            "Epoch 100/100\n",
            "1154/1154 [==============================] - 6s 5ms/step - loss: 0.0658 - accuracy: 0.9782 - val_loss: 0.1133 - val_accuracy: 0.9778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "319DAwUG2IPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "d779ca77-de78-4485-f65e-6f5ff4fec780"
      },
      "source": [
        "# Plot the classifier loss and accuracy curves for the training and validation data\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(history1.history['loss'], color='b', label=\"Training loss\")\n",
        "ax[0].plot(history1.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "ax[0].set(xlabel='epochs', ylabel='loss')\n",
        "\n",
        "ax[1].plot(history1.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax[1].plot(history1.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)\n",
        "ax[1].set(xlabel='epochs', ylabel='accuracy')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'accuracy'), Text(0.5, 0, 'epochs')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81R-8-oq2iKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2394c88a-ab6a-4383-cdcf-a6dd76544fc4"
      },
      "source": [
        "#Calculate the accuracy for test data\n",
        "y_test=AllNT2DataSet['ID']\n",
        "X_test=AllNT2DataSet.drop(['ID'],axis=1).astype(float)\n",
        "y_test=y_test.astype(int)-1\n",
        "y_test = to_categorical(y_test)\n",
        "#print(y_test)\n",
        "Classfier.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
        "loss, accuracy = Classfier.evaluate(X_test, y_test)\n",
        "#print('Test score:', score)\n",
        "print('Accuracy:', accuracy)\n",
        "print('Loss:', loss)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 1s 1ms/step - loss: 1.5216 - accuracy: 0.7607\n",
            "Accuracy: 0.7649006843566895\n",
            "Loss: 1.4227142333984375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}